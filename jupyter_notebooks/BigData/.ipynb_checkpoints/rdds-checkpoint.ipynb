{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba73de7d-f3ec-41a3-afd8-7e6e46dac685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sample data created in data/sample.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "# Create sample text file\n",
    "sample_text = [\n",
    "    \"Apache Spark is fast\",\n",
    "    \"Spark provides RDDs\",\n",
    "    \"RDDs are fundamental\",\n",
    "    \"Processing big data\"\n",
    "]\n",
    "\n",
    "# Write sample data to file\n",
    "with open('data/sample.txt', 'w') as f:\n",
    "    f.write('\\n'.join(sample_text))\n",
    "\n",
    "print(\"✅ Sample data created in data/sample.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf9fb3f-3555-4df0-8498-949f31adb890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session with cluster master\n",
    "spark = SparkSession.builder.appName(\"SparkExample\").master(\"spark://spark-master:7077\").config(\"spark.driver.memory\", \"1g\").config(\"spark.executor.memory\", \"1g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b31b2e-7482-4f3f-b621-d7c49e1d4bc0",
   "metadata": {},
   "source": [
    "# Understanding RDDs\n",
    "\n",
    "An RDD (Resilient Distributed Dataset) is the foundation of Apache Spark. Think of it as a distributed collection of elements that can be processed in parallel. RDDs are immutable, meaning once created, they cannot be changed - instead, each transformation creates a new RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016a5b7e-a648-4401-abb9-d1054176d5dc",
   "metadata": {},
   "source": [
    "## Creating RDDs\n",
    "\n",
    "We can create RDDs in several ways. Let's look at each method with examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f503d7c-a2b4-4631-b8ba-7973e9625749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data: [1, 2, 3, 4, 5]\n",
      "RDD content: [1, 2, 3, 4, 5]\n",
      "\n",
      "Squared numbers: [1, 4, 9, 16, 25]\n",
      "Even squares: [4, 16]\n"
     ]
    }
   ],
   "source": [
    "# Create our first RDD from a list\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd1 = spark.sparkContext.parallelize(data)\n",
    "\n",
    "print(\"Original data:\", data)\n",
    "print(\"RDD content:\", rdd1.collect())\n",
    "\n",
    "# Try some transformations\n",
    "squared = rdd1.map(lambda x: x * x)\n",
    "print(\"\\nSquared numbers:\", squared.collect())\n",
    "\n",
    "# Filter for even numbers\n",
    "evens = squared.filter(lambda x: x % 2 == 0)\n",
    "print(\"Even squares:\", evens.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739d89ad-41c1-4909-8806-228db8dd2685",
   "metadata": {},
   "source": [
    "## Working with Text Data\n",
    "Let's create and process some text data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f1b9766-6a19-45ae-a716-0a006beef909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our text data:\n",
      "  Apache Spark is fast\n",
      "  Spark has RDDs\n",
      "  RDDs are fundamental\n"
     ]
    }
   ],
   "source": [
    "# Create an RDD directly from a list of strings\n",
    "texts = [\n",
    "    \"Apache Spark is fast\",\n",
    "    \"Spark has RDDs\",\n",
    "    \"RDDs are fundamental\"\n",
    "]\n",
    "text_rdd = spark.sparkContext.parallelize(texts)\n",
    "\n",
    "print(\"Our text data:\")\n",
    "for line in text_rdd.collect():\n",
    "    print(f\"  {line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "997548ac-b2ef-4e9a-8f85-759c6a81856d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Individual words: ['Apache', 'Spark', 'is', 'fast', 'Spark', 'has', 'RDDs', 'RDDs', 'are', 'fundamental']\n",
      "\n",
      "Word-count pairs: [('Apache', 1), ('Spark', 1), ('is', 1), ('fast', 1), ('Spark', 1), ('has', 1), ('RDDs', 1), ('RDDs', 1), ('are', 1), ('fundamental', 1)]\n",
      "\n",
      "Word frequencies:\n",
      "  Apache: 1\n",
      "  Spark: 2\n",
      "  is: 1\n",
      "  are: 1\n",
      "  fundamental: 1\n",
      "  fast: 1\n",
      "  has: 1\n",
      "  RDDs: 2\n"
     ]
    }
   ],
   "source": [
    "# Let's count words\n",
    "words = text_rdd.flatMap(lambda line: line.split())\n",
    "print(\"\\nIndividual words:\", words.collect())\n",
    "\n",
    "# Create word-count pairs\n",
    "word_pairs = words.map(lambda word: (word, 1))\n",
    "print(\"\\nWord-count pairs:\", word_pairs.collect())\n",
    "\n",
    "# Count word occurrences\n",
    "word_counts = word_pairs.reduceByKey(lambda a, b: a + b)\n",
    "print(\"\\nWord frequencies:\")\n",
    "for word, count in word_counts.collect():\n",
    "    print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b11ebb5-36eb-443c-8c60-99d353f9e16d",
   "metadata": {},
   "source": [
    "## Understanding Transformations vs Actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc07daf0-eb3f-406f-ac68-a4be2e154c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with numbers: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "\n",
      "Transformations defined but not yet executed...\n",
      "\n",
      "Now executing transformations through actions:\n",
      "Doubled numbers: [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n",
      "Numbers > 10: [12, 14, 16, 18, 20]\n",
      "Count of numbers > 10: 5\n"
     ]
    }
   ],
   "source": [
    "# Create a base RDD\n",
    "numbers = range(1, 11)  # 1 to 10\n",
    "base_rdd = spark.sparkContext.parallelize(numbers)\n",
    "\n",
    "print(\"Starting with numbers:\", list(numbers))\n",
    "\n",
    "# TRANSFORMATIONS (lazy operations)\n",
    "# These won't execute until an action is called\n",
    "doubled = base_rdd.map(lambda x: x * 2)\n",
    "filtered = doubled.filter(lambda x: x > 10)\n",
    "\n",
    "print(\"\\nTransformations defined but not yet executed...\")\n",
    "\n",
    "# ACTIONS (trigger execution)\n",
    "print(\"\\nNow executing transformations through actions:\")\n",
    "print(\"Doubled numbers:\", doubled.collect())\n",
    "print(\"Numbers > 10:\", filtered.collect())\n",
    "print(\"Count of numbers > 10:\", filtered.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06867a9a-76e2-4646-bf76-149aec986886",
   "metadata": {},
   "source": [
    "# Advanced operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5afd71-3165-4e4b-b66f-5affc5b87299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try some more complex operations\n",
    "complex_data = [\n",
    "    (\"Spark\", 100),\n",
    "    (\"Hadoop\", 80),\n",
    "    (\"Spark\", 90),\n",
    "    (\"Flink\", 95)\n",
    "]\n",
    "complex_rdd = spark.sparkContext.parallelize(complex_data)\n",
    "\n",
    "# GroupByKey operation\n",
    "grouped = complex_rdd.groupByKey().mapValues(list)\n",
    "print(\"Grouped by key:\")\n",
    "for k, v in grouped.collect():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# ReduceByKey operation\n",
    "summed = complex_rdd.reduceByKey(lambda a, b: a + b)\n",
    "print(\"\\nSummed by key:\")\n",
    "for k, v in summed.collect():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f67bcf-4239-4ee5-a75d-90fffd681886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create a larger dataset for testing\n",
    "big_data = list(range(1, 10000001))  # 1 to 10 million\n",
    "big_rdd = spark.sparkContext.parallelize(big_data)\n",
    "\n",
    "def process_data(rdd, use_cache=False):\n",
    "    # Complex transformation chain\n",
    "    result = rdd.map(lambda x: x * x) \\\n",
    "                .filter(lambda x: x % 2 == 0) \\\n",
    "                .map(lambda x: x / 2)\n",
    "    \n",
    "    if use_cache:\n",
    "        result.cache()\n",
    "    \n",
    "    # Force execution and measure time\n",
    "    start = time.time()\n",
    "    count = result.count()\n",
    "    duration = time.time() - start\n",
    "    \n",
    "    print(f\"Count: {count:,}\")\n",
    "    print(f\"Time taken: {duration:.2f} seconds\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"Without caching:\")\n",
    "result1 = process_data(big_rdd)\n",
    "\n",
    "print(\"\\nWith caching:\")\n",
    "result2 = process_data(big_rdd, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c486692-dfa6-41eb-bcff-1bed8bb29ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample sentences\n",
    "sentences = [\n",
    "    \"Spark is a fast and general engine for large-scale data processing\",\n",
    "    \"Spark provides an interface for programming entire clusters\",\n",
    "    \"RDDs are the fundamental data structure of Spark\",\n",
    "    \"Spark supports multiple programming languages\"\n",
    "]\n",
    "\n",
    "# Create RDD\n",
    "lines_rdd = spark.sparkContext.parallelize(sentences)\n",
    "\n",
    "print(\"Original sentences:\")\n",
    "for line in lines_rdd.collect():\n",
    "    print(f\"  {line}\")\n",
    "\n",
    "# Step 1: Split into words\n",
    "words = lines_rdd.flatMap(lambda line: line.lower().split())\n",
    "print(\"\\nWords (first 10):\", words.take(10))\n",
    "\n",
    "# Step 2: Create word-count pairs\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "print(\"\\nWord-count pairs (sample):\", pairs.take(5))\n",
    "\n",
    "# Step 3: Count occurrences\n",
    "counts = pairs.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Sort by frequency\n",
    "sorted_counts = counts.sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "print(\"\\nWord frequencies (top 10):\")\n",
    "for word, count in sorted_counts.take(10):\n",
    "    print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056522d9-879c-450a-9589-c645003439a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD with specific number of partitions\n",
    "rdd = spark.sparkContext.parallelize(range(100), 4)\n",
    "\n",
    "print(f\"Number of partitions: {rdd.getNumPartitions()}\")\n",
    "\n",
    "# See data in each partition\n",
    "def print_partition(iterator):\n",
    "    return [f\"Partition data: {list(iterator)}\"]\n",
    "\n",
    "partitioned_data = rdd.mapPartitions(print_partition)\n",
    "print(\"\\nData by partition:\")\n",
    "for partition in partitioned_data.collect():\n",
    "    print(partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dff2b07-2a83-4a40-93d5-6843fc1e4286",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
